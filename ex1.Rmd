---
title: "R Notebook"
output: html_notebook
---

## Ex1

```{r}
# Classification with knn
install.packages("pacman")

# Install class for knn classification
pacman::p_load("class")

# Load Data
dataIris <- read.csv("C:\\Users\\endri\\amas\\lec10\\Data for PS10-20230624\\iris.csv", header = TRUE)
colnames(dataIris)
head(dataIris)

# Normalization - Important for different ranges
# Define the function for normalization
normal <- function(x) {
  norm <- ((x - min(x))/(max(x) - min(x)))
  return(norm)
}

# Apply function to data frame without ID and default
normalDataIris <- as.data.frame(lapply(dataIris[, 1:4], normal))
head(normalDataIris)

# Put outcome variable back on and rename
normalDataIris <- cbind(normalDataIris, dataIris[, 5])
names(normalDataIris)[5] <- "Species"

# Split Data
set.seed(800)
normalDataIris.split <- sample(2, nrow(normalDataIris), replace = TRUE, prob = c(0.8, 0.2))

# Create training and testing dataset without outcome labels
normalDataIris.train.labels <- normalDataIris[normalDataIris.split == 1, 5]
normalDataIris.test.labels <- normalDataIris[normalDataIris.split == 2, 5]

# Create outcome labels
normalDataIris.train <- normalDataIris[normalDataIris.split == 1, 1:4]
normalDataIris.test <- normalDataIris[normalDataIris.split == 2, 1:4]

# Build and test classifier
normalDataIris.pred <- knn(train = normalDataIris.train,
                           test = normalDataIris.test,
                           cl = normalDataIris.train.labels, # true class
                           k = 3) # n neighbors

# Compare predicted outcomes to observed outcome
table(normalDataIris.pred, normalDataIris.test.labels)

# Clean up
rm(list = ls())

```

## Ex2

```{r}
# Load required packages
install.packages(c("rpart", "naivebayes", "e1071", "caret", "ggplot2", "corrplot"))
library(rpart)
library(naivebayes)
library(e1071)
library(caret)
library(ggplot2)
library(corrplot)

# Load the Iris dataset
dataIris <- read.csv("C:\\Users\\endri\\amas\\lec10\\Data for PS10-20230624\\iris.csv", header = TRUE)

# Split the data into training and testing sets
set.seed(800)
train_indices <- sample(1:nrow(dataIris), 0.8 * nrow(dataIris))
train_data <- dataIris[train_indices, ]
test_data <- dataIris[-train_indices, ]

# Convert the Species variable to a factor in both train_data and test_data
train_data$Species <- factor(train_data$Species)
test_data$Species <- factor(test_data$Species)

# Align factor levels between predicted values and reference values
levels(test_data$Species) <- levels(train_data$Species)

# Decision Tree Model
dt_model <- rpart(Species ~ ., data = train_data)
dt_predictions <- predict(dt_model, test_data, type = "class")
dt_accuracy <- confusionMatrix(dt_predictions, test_data$Species)$overall["Accuracy"]

# Naive Bayes Model
nb_model <- naive_bayes(Species ~ ., data = train_data)
nb_predictions <- predict(nb_model, test_data)
nb_accuracy <- confusionMatrix(nb_predictions, test_data$Species)$overall["Accuracy"]

# Support Vector Machines Model
svm_model <- svm(Species ~ ., data = train_data)
svm_predictions <- predict(svm_model, test_data)
svm_accuracy <- confusionMatrix(svm_predictions, test_data$Species)$overall["Accuracy"]

# Compare model performances
models <- c("Decision Tree", "Naive Bayes", "Support Vector Machines")
accuracies <- c(dt_accuracy, nb_accuracy, svm_accuracy)
comparison <- data.frame(models, accuracies)

# Visualize model performances with boxplot
ggplot(comparison, aes(x = models, y = accuracies)) +
  geom_boxplot() +
  labs(title = "Comparison of Model Performances",
       x = "Models", y = "Accuracy")

# Scatter plot of two features
ggplot(dataIris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  labs(title = "Scatter Plot of Sepal Length and Width",
       x = "Sepal Length", y = "Sepal Width")

# Correlation matrix
cor_matrix <- cor(dataIris[, 1:4])
corrplot(cor_matrix, method = "color", type = "upper",
         tl.col = "black", tl.srt = 45)

# Confusion matrix for Decision Tree
dt_confusion <- confusionMatrix(dt_predictions, test_data$Species)
print(dt_confusion)

```

# Ex3

## a) Data Reduction

```{r}
# Load required package
library(caret)

# Load the 'wine-quality-red.csv' dataset
data <- read.csv("C:\\Users\\endri\\amas\\lec10\\Data for PS10-20230624\\wine-quality-red.csv", header = TRUE)

# Preprocess the data if necessary (e.g., handle missing values, scale/normalize)

# Perform PCA
pca_model <- prcomp(data[, -ncol(data)], scale. = TRUE)

# Explore PCA results
summary(pca_model)

```

## b) Cluster analysis

```{r}
# Load required package
library(cluster)

# Load the 'wine-quality-red.csv' dataset


# Preprocess the data if necessary

# Perform k-Means clustering
k <- 3  # Number of clusters
kmeans_model <- kmeans(data[, -ncol(data)], centers = k)

# Get cluster assignments
cluster_assignments <- kmeans_model$cluster

# Explore clustering results
table(cluster_assignments)

```

## c) Classification

```{r}
# Load required package
library(caret)
library(ggplot2)
library(reshape2)


# Preprocess the data if necessary

# Convert the 'quality' variable into a factor
data$quality <- factor(data$quality)

# Split the data into training and testing sets
set.seed(123)
train_indices <- createDataPartition(data$quality, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]

# Train a Logistic Regression model
model <- train(quality ~ ., data = train_data, method = "multinom")

# Make predictions on the test data
predictions <- predict(model, newdata = test_data)

# Create a confusion matrix
cm <- table(predictions, test_data$quality)

print(cm)

accuracy <- sum(diag(cm)) / sum(cm)
print(paste("Accuracy (R):", accuracy))
```

# Ex4

```{r}
# Load required libraries
library(ggplot2)
library(dplyr)
library(gridExtra)

# Read the data from the CSV file
combined_data <- read.csv("C:\\Users\\endri\\amas\\lec10\\Data for PS10-20230624\\ex4\\all.csv", header = TRUE)

# Convert the 'Month' column to Date format
combined_data$Month <- as.Date(combined_data$Month, format = "%m/%d/%Y")

# Convert 'avocado' column to numeric (remove "<" character)
combined_data$avocado <- as.numeric(gsub("<", "", combined_data$avocado))

# Create individual plots for each search term
plot_sushi <- ggplot(combined_data, aes(x = Month, y = sushi)) +
  geom_line() +
  labs(x = "Month", y = "Search Volume", title = "Sushi: (Germany)")

plot_avocado <- ggplot(combined_data, aes(x = Month, y = avocado)) +
  geom_line() +
  labs(x = "Month", y = "Search Volume", title = "Avocado: (Germany)")

plot_pizza <- ggplot(combined_data, aes(x = Month, y = pizza)) +
  geom_line() +
  labs(x = "Month", y = "Search Volume", title = "Pizza: (Germany)")

plot_donner <- ggplot(combined_data, aes(x = Month, y = donner)) +
  geom_line() +
  labs(x = "Month", y = "Search Volume", title = "Donner: (Germany)")

plot_wurst <- ggplot(combined_data, aes(x = Month, y = wurst)) +
  geom_line() +
  labs(x = "Month", y = "Search Volume", title = "Wurst: (Germany)")

# Arrange the plots in a grid
combined_plots <- grid.arrange(plot_sushi, plot_avocado, plot_pizza, plot_donner, plot_wurst, ncol = 2)

# Get the overall most popular search term
most_popular_term <- names(combined_data)[which.max(colSums(combined_data[, c("sushi", "avocado", "pizza", "donner", "wurst")]))]

# Print the most popular term
cat("Overall Most Popular Search Term:", most_popular_term, "\n")

# Print the month with the highest search volume for each term
cat("Month with Highest Search Volume:\n")
cat("Sushi:", combined_data$Month[which.max(combined_data$sushi)], "\n")
cat("Avocado:", combined_data$Month[which.max(combined_data$avocado)], "\n")
cat("Pizza:", combined_data$Month[which.max(combined_data$pizza)], "\n")
cat("Donner:", combined_data$Month[which.max(combined_data$donner)], "\n")
cat("Wurst:", combined_data$Month[which.max(combined_data$wurst)], "\n")

# Display the combined plots
print(combined_plots)

```
